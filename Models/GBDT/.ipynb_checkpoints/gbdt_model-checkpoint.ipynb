{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "TrainTestDataDir = '/home/songyue/homeCredit/HomeCreditDefaultRisk/Data/TrainTestData'\n",
    "modelDir = '/home/songyue/homeCredit/HomeCreditDefaultRisk/GBDT'\n",
    "\n",
    "def loadData(DataName):\n",
    "    DataPath = os.path.join(TrainTestDataDir, DataName)\n",
    "    if not os.path.exists(DataPath):\n",
    "        print('%s does not exist!' % DataPath)\n",
    "        return\n",
    "    OriginData = pd.read_csv(DataPath, index_col=0)\n",
    "    OriginData = OriginData.sample(frac=1)  # 打乱顺序后返回\n",
    "    return OriginData\n",
    "\n",
    "\n",
    "def NormalData(TrainData, TestData):\n",
    "    # 对一些列的均值大于100的进行归一化处理\n",
    "    AllData = TrainData.append(TestData)\n",
    "    for col in AllData.columns:\n",
    "        if abs(AllData[col].mean())>1:\n",
    "            scaler = StandardScaler().fit(np.atleast_2d(AllData[col]).T)\n",
    "            TrainData[col] = scaler.transform(np.atleast_2d(TrainData[col]).T)\n",
    "    return TrainData\n",
    "ValidData = loadData('validation.csv')\n",
    "ValidID = ValidData.SK_ID_CURR.values\n",
    "# ValidData.drop('SK_ID_CURR', axis=1, inplace=True)\n",
    "TrainData = loadData('train.csv')\n",
    "TrainData.reset_index(drop=True, inplace=True)\n",
    "TrainDataPos = TrainData[TrainData.TARGET==1]\n",
    "TrainDataNeg = TrainData[TrainData.TARGET==0]\n",
    "TrainDataTest = TrainDataPos.append(TrainDataNeg.iloc[:len(TrainDataNeg)//10,:])\n",
    "TrainDataTest.reset_index(drop=True,inplace=True)\n",
    "print(len(TrainDataTest))\n",
    "# TrainDataTest.head()\n",
    "TrainDataTest.drop('SK_ID_CURR', axis=1, inplace=True)\n",
    "ValidData.drop('SK_ID_CURR', axis=1, inplace=True)\n",
    "def NormalData2(Data):\n",
    "    for col in Data.columns:\n",
    "        if abs(Data[col].mean())>1:\n",
    "            scaler = StandardScaler().fit(np.atleast_2d(Data[col]).T)\n",
    "            Data[col] = scaler.transform(np.atleast_2d(Data[col]).T)\n",
    "    return Data\n",
    "def missing_values_table(df):\n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0:'Missing Values',1:'% of Total Values'})\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "gbdt=GradientBoostingClassifier(max_depth=8,max_features=20,n_estimators=110)\n",
    "ModelDir = 'models'\n",
    "ModelName = 'train1'\n",
    "TrainTargetTest = TrainDataTest.TARGET\n",
    "ValTargetTest = ValidData.TARGET\n",
    "# print(TrainTargetTest)\n",
    "print(len(TrainTargetTest))\n",
    "print(len(ValTargetTest))\n",
    "TrainDatatest = NormalData2(TrainDataTest)\n",
    "ValidDatatest = NormalData2(ValidData)\n",
    "TrainDatatest.head()\n",
    "ValidDatatest.head()\n",
    "X_train = TrainDatatest.drop('TARGET', axis=1).values\n",
    "# X_train = X_train.values\n",
    "y_train = TrainTargetTest.values\n",
    "X_val = ValidDatatest.drop('TARGET', axis=1).values\n",
    "y_val = ValTargetTest.values\n",
    "gbdt.fit(X_train,y_train)\n",
    "pred = gbdt.predict(X_val)\n",
    "pd.crosstab(y_val,pred)\n",
    "m_dir = os.path.join(modelDir,ModelDir)\n",
    "joblib.dump(gbdt,os.path.join(m_dir,'gbdt3.m'))\n",
    "print('acc: ',accuracy_score(y_val,pred))\n",
    "print('roc: ',roc_auc_score(y_val,pred))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_test1 = {'n_estimators':np.arange(20,81,10)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=300,\n",
    "                                  min_samples_leaf=20,max_depth=8,max_features='sqrt', subsample=0.8,random_state=10), \n",
    "                       param_grid = param_test1, scoring='roc_auc',iid=False,cv=5)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_test1 = {'n_estimators':np.arange(20,81,10)}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=300,\n",
    "                                  min_samples_leaf=20,max_depth=8,max_features='sqrt', subsample=0.8,random_state=10), \n",
    "                       param_grid = param_test1, scoring='roc_auc',iid=False,cv=5)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "param_test2 = {'n_estimators':np.arange(90,120,10)}\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=300,\n",
    "                                  min_samples_leaf=20,max_depth=8,max_features='sqrt', subsample=0.8,random_state=10), \n",
    "                       param_grid = param_test2, scoring='roc_auc',iid=False,cv=5)\n",
    "gsearch2.fit(X_train,y_train)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n",
    "param_test3 = {'n_estimators':np.arange(100,151,10),\n",
    "               'max_features':np.arange(7,25,2),\n",
    "                'max_depth':np.arange(3,14,2)}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=300,\n",
    "                                  min_samples_leaf=20,subsample=0.8,random_state=10), \n",
    "                       param_grid = param_test3, scoring='roc_auc',iid=False,cv=5)\n",
    "gsearch3.fit(X_train,y_train)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_\n",
    "param_test3 = {'n_estimators':np.arange(100,151,10),\n",
    "               'max_features':np.arange(7,25,2),\n",
    "                'max_depth':np.arange(3,14,2)}\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=300,\n",
    "                                  min_samples_leaf=20,subsample=0.8,random_state=10), \n",
    "                       param_grid = param_test3, scoring='roc_auc',iid=False,cv=5)\n",
    "gsearch3.fit(X_train,y_train)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练10个模型\n",
    "\n",
    "n = 10\n",
    "\n",
    "for i in range(10):\n",
    "    batch_size = len(TrainDataNeg)//10\n",
    "    if i == 9:\n",
    "        trainData = TrainDataPos.append(TrainDataNeg.iloc[9*batch_size:,:])\n",
    "    else:\n",
    "        trainData = TrainDataPos.append(TrainDataNeg.iloc[i*batch_size:(i+1)*batch_size,:])\n",
    "    trainData.reset_index(drop=True,inplace=True)\n",
    "    trainData.drop('SK_ID_CURR', axis=1, inplace=True)\n",
    "    gbdt=GradientBoostingClassifier(max_depth=gsearch3.best_params_['max_depth'],\n",
    "                                    max_features=gsearch3.best_params_['max_features'],\n",
    "                                    n_estimators=gsearch3.best_params_['n_estimators'])\n",
    "    ModelDir = 'models'\n",
    "    ModelName = 'train1'\n",
    "    trainTarget = trainData.TARGET\n",
    "    trainData = NormalData2(trainData)\n",
    "    X_train = trainData.drop('TARGET', axis=1).values\n",
    "    # X_train = X_train.values\n",
    "    y_train = trainData.values\n",
    "#     X_val = ValidDatatest.drop('TARGET', axis=1).values\n",
    "#     y_val = ValTargetTest.values\n",
    "    gbdt.fit(X_train,y_train)\n",
    "    pred = gbdt.predict(X_val)\n",
    "    pd.crosstab(y_val,pred)\n",
    "    m_dir = os.path.join(modelDir,ModelDir)\n",
    "    model_name = 'gbdt_%s.m' % str(i+1)\n",
    "    joblib.dump(gbdt,os.path.join(m_dir,model_name))\n",
    "    print('acc: ',accuracy_score(y_val,pred))\n",
    "    print('roc: ',roc_auc_score(y_val,pred))\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
