{
 "cells": [],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
etadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/share/anaconda2/envs/py3/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 4 sec\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/share/anaconda2/envs/py3/lib/python3.5/site-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/share/anaconda2/envs/py3/lib/python3.5/site-packages/ipykernel_launcher.py:63: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b5620080ccbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mneg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUMB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUMB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mx0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfd\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y0' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "start_time = time.time()\n",
    "input_dir = '/home/songyue/homeCredit/HomeCreditDefaultRisk/Data/OriginData'\n",
    "\n",
    "\n",
    "TrainTestDataDir = '/home/songyue/homeCredit/HomeCreditDefaultRisk/Data/TrainTestData'\n",
    "def loadData(DataName):\n",
    "    DataPath = os.path.join(TrainTestDataDir, DataName)\n",
    "    if not os.path.exists(DataPath):\n",
    "        print('%s does not exist!' % DataPath)\n",
    "        return\n",
    "    OriginData = pd.read_csv(DataPath, index_col=0)\n",
    "    OriginData = OriginData.sample(frac=1)  # 打乱顺序后返回\n",
    "    return OriginData\n",
    "\n",
    "\n",
    "def NormalData(TrainData, TestData):\n",
    "    # 对一些列的均值大于100的进行归一化处理\n",
    "    AllData = TrainData.append(TestData)\n",
    "    for col in AllData.columns:\n",
    "        if abs(AllData[col].mean())>1:\n",
    "            scaler = StandardScaler().fit(np.atleast_2d(AllData[col]).T)\n",
    "            TrainData[col] = scaler.transform(np.atleast_2d(TrainData[col]).T)\n",
    "    return TrainData\n",
    "\n",
    "# Re-separate into train and testModelDir = 'models'\n",
    "ModelName = 'train4'\n",
    "TrainDataName = ModelName + '.csv'\n",
    "TestDataName = 'test.csv'\n",
    "TrainData = loadData(TrainDataName)\n",
    "TestData = loadData(TestDataName)\n",
    "# 更改训练样本的index\n",
    "TrainData.set_index('SK_ID_CURR', inplace=True)\n",
    "TestData.set_index('SK_ID_CURR', inplace=True)\n",
    "TrainTarget = TrainData.TARGET\n",
    "TrainData.drop('TARGET', axis=1, inplace=True)\n",
    "TrainData = NormalData(TrainData, TestData)\n",
    "# 划分训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(TrainData.values,\n",
    "                                                      TrainTarget.values, test_size=0.1,\n",
    "                                                      random_state=711)\n",
    "# 将他们转化为torch tensor\n",
    "train_x = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "train_y = torch.from_numpy(y_train).type(torch.FloatTensor)\n",
    "val_x = Variable(torch.from_numpy(X_val).type(torch.FloatTensor), volatile=True).cuda()\n",
    "val_y = Variable(torch.from_numpy(y_val).type(torch.FloatTensor), volatile=True).cuda()\n",
    "\n",
    "\n",
    "print('Time elapsed %.0f sec'%(time.time()-start_time))\n",
    "print('Starting training...')\n",
    "\n",
    "# define train parameters\n",
    "L2c = 4e-4                    # loss, with L2\n",
    "lr0 = 0.02                    # starting learning rate\n",
    "lr_decay = 0.90               # lr decay rate\n",
    "iterations = 41               # full passes over data\n",
    "ROWS = train_x.shape[0]      # rows in input data\n",
    "VARS = train_x.shape[1]      # vars used in the model\n",
    "NUMB = 10000                  # batch size\n",
    "NN = int(ROWS/NUMB)           # number of batches\n",
    "\n",
    "# define the model\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "x  = tf.placeholder(tf.float32, [None, VARS])\n",
    "\n",
    "# model: logistic + 1 hidden layer\n",
    "W      = tf.Variable(tf.truncated_normal([VARS,1],mean=0.0,stddev=0.001),dtype=np.float32)\n",
    "NUML1  = 64\n",
    "W1     = tf.Variable(tf.truncated_normal([VARS,NUML1],mean=0.0,stddev=0.0001),dtype=np.float32)\n",
    "W1f    = tf.Variable(tf.truncated_normal([NUML1,1],mean=0.0,stddev=0.0001),dtype=np.float32)\n",
    "logit1 = tf.matmul( x, W ) + tf.matmul(tf.nn.relu(tf.matmul( x, W1 )), W1f)\n",
    "y      = tf.nn.sigmoid( logit1 )\n",
    "\n",
    "# loss/optimizer\n",
    "loss0 = tf.reduce_mean( (y_-y)*(y_-y) )\n",
    "loss1 = L2c * (tf.nn.l2_loss( W ) + tf.nn.l2_loss( W1 ) + tf.nn.l2_loss( W1f ))\n",
    "loss  = loss0 + loss1\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(lr0, global_step, NN, lr_decay)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss,global_step=global_step)\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# main training loop\n",
    "# y0=train_y.values.astype(np.float32)\n",
    "# x0=train_x.values.astype(np.float32)\n",
    "# del train_df\n",
    "# gc.collect()\n",
    "y0_1=np.where(train_x[0:int(NN*0.8)*NUMB] == 1)[0] # reserve last 20% for testing\n",
    "y0_0=np.where(train_y[0:int(NN*0.8)*NUMB] == 0)[0] # reserve last 20% for testing\n",
    "for i in range(iterations):\n",
    "    for j in range(int(NN*0.8)): # reserve last 20% for testing\n",
    "        pos_ratio = 0.5\n",
    "        pos_idx = np.random.choice(y0_1, size=int(np.round(NUMB*pos_ratio)))\n",
    "        neg_idx = np.random.choice(y0_0, size=int(np.round(NUMB*(1-pos_ratio))))\n",
    "        idx = np.concatenate([pos_idx, neg_idx])\n",
    "        fd = {y_: train_y[idx].reshape(NUMB,1),x:  x0[idx,:]}\n",
    "        _= sess.run( [train_step], feed_dict=fd )\n",
    "    if i%10 == 0:\n",
    "        # get area under the ROC curve\n",
    "        fd   = {y_: train_y.reshape(train_y.shape[0],1),x: train_x}\n",
    "        y1   = sess.run( y, feed_dict=fd )\n",
    "        lim  = int(NN*0.8) * NUMB\n",
    "        auc1 = roc_auc_score(train_y[0:lim],y1[0:lim,0])\n",
    "        auc2 = roc_auc_score(train_y[lim:train_y.shape[0]],y1[lim:train_y.shape[0],0])\n",
    "        print('iteration %d, auc train/validatet %.5f/%.5f'%(i,auc1,auc2))\n",
    "\n",
    "#Predict on test set and create submission\n",
    "#x0     = val_x.values.astype(np.float32)\n",
    "#fd     = {y_: np.zeros([x0.shape[0],1]),x: x0}\n",
    "#y_pred = sess.run( y, feed_dict=fd )\n",
    "#out_df = pd.DataFrame({'SK_ID_CURR': meta_df[len_train:], 'TARGET': y_pred[:,0]})\n",
    "#out_df.to_csv('submission.csv', index=False)\n",
    "#print('Time elapsed %.0f sec'%(time.time()-start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
